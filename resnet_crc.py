# -*- coding: utf-8 -*-
"""ResNet_CRC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C0gABa0qr2TjY0Oy9EiROudolCVtUHXW

## DATA DOWNLOAD LINK
"""

# 해당 코드는 코랩용 코드입니다. 데이터를 갖고 실험을 진행하는 경우 필요 없습니다. 하단 이 블록의 코드를 제외한 하단 코드들은 조교님들이 돌리기에 방해되지 않도록 경로를 수정했습니다.
from urllib import request 
import os
import zipfile

data_name = ['CRC_DX_TRAIN_MSIMUT', 'CRC_DX_TRAIN_MSS', 'CRC_DX_TEST_MSS', 'CRC_DX_TEST_MSIMUT'] 
front_link = 'https://zenodo.org/record/2530835/files/' 
back_link = '.zip?download=1' 

# 폴더 디렉토리 생성 
os.makedirs('/content/TCGA_DATA') 
os.makedirs('/content/TCGA_DATA/CRC_TRAIN') 
os.makedirs('/content/TCGA_DATA/CRC_TEST')

for idx, data_type in enumerate(data_name):
  # 코랩 아니면 아래 경로 수정
  if idx <= 1:
    os.chdir('/content/TCGA_DATA/CRC_TRAIN')
  else:
    os.chdir('/content/TCGA_DATA/CRC_TEST')
  
  link = front_link + data_type + back_link
  request.urlretrieve(link,data_type) 
  zipfile.ZipFile(data_type).extractall()  
  print('One Done')

"""## DATA PATH 가져오기 """

from glob import glob
DATA_PATH_TRAIN_LIST = glob('./TCGA_DATA/CRC_TRAIN/*/*.png') 
DATA_PATH_TEST_LIST = glob('./TCGA_DATA/CRC_TEST/*/*.png')

"""## Custom DataSet """

from torch.utils.data import Dataset, DataLoader
from skimage import io, transform

class CRC_DataSet(Dataset):
    #data_path_list - 이미지 path 전체 리스트
    #label - 이미지 ground truth
    def __init__(self, data_path_list, classes, transform=None):
        self.path_list = data_path_list
        self.label = []
        for path in data_path_list:
            self.label.append(path.split('/')[-2])
        self.transform = transform
        self.classes = classes
    
    def __len__(self):
        return len(self.path_list)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        image = io.imread(self.path_list[idx])
        if self.transform is not None:
            image = self.transform(image)
        return image, self.classes.index(self.label[idx])

"""## Data Loader"""

import torch
from torchvision import transforms 

classes = ('MSIMUT', 'MSS')

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TRAIN_LIST, 
        classes,
        transform=transform
    ),
    batch_size=4,
    shuffle = True
)

testloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TEST_LIST, 
        classes,
        transform=transform
    ),
    batch_size=4,
    shuffle = False
)

"""## Glasses Module 설치를 위한 패키지"""
"""
!pip install git+https://github.com/FrancescoSaverioZuppichini/glasses
!pip install huggingface_hub
!pip install torchinfo
!pip install rich
!pip install einops
"""
"""## Fine Tuning with Resnet 18"""
from glasses.models import ResNet 
import torch.nn.functional as F

import random
SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

"""## HyperParameter

"""

lr = 0.0001
epochs = 20
batch_size = 400

"""## Data Loader

"""

classes = ('MSIMUT', 'MSS')

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TRAIN_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

testloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TEST_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = False
)

"""## Train Model

"""

model = ResNet.resnet18(n_classes=2).to(device)

criterion = torch.nn.CrossEntropyLoss().to(device)  
optimizer = torch.optim.Adam(model.parameters(), lr= lr, weight_decay=0.3)

for epoch in range(epochs):
    avg_cost = 0
    train_loss = 0
    train_total = 0
    train_correct = 0
    model.train()
    for X, Y in trainloader: 
        X = X.to(device)
        Y = Y.to(device) 
        
        optimizer.zero_grad() 

        model_predict  = model(X)
        loss = criterion(model_predict, Y) 
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        _, predicted = model_predict.max(1)
        train_total += Y.size(0) 
        train_correct += predicted.eq(Y).sum().item()
    train_acc = train_correct / train_total
    valid_loss = 0
    valid_total = 0
    valid_correct = 0
    model.eval() 
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, labels)
            valid_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            valid_total += labels.size(0)
            valid_correct += (predicted == labels).sum().item()
    valid_acc = valid_correct / valid_total
    print('[%d/%d] TrainLoss: %.3f, ValLoss: %.3f | TrainAcc: %.2f, ValAcc: %.2f' % (epoch, epochs, train_loss, valid_loss, train_acc, valid_acc))

"""## CRC_TEST data set에서 MSS 데이터가 MSIMUT 데이터보다 훨씬 많음 여기서 Under Sampling 수행 해봄."""

DATA_PATH_TRAIN_LIST = glob('./TCGA_DATA/CRC_TRAIN/*/*.png') 
DATA_PATH_TEST_LIST = glob('./TCGA_DATA/CRC_TEST/MSS/*.png')[:28335] + glob('./TCGA_DATA/CRC_TEST/MSIMUT/*.png')

lr = 0.00001
epochs = 30
batch_size = 400

import torch
from torchvision import transforms 

classes = ('MSIMUT', 'MSS')

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TRAIN_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

testloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TEST_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

model = ResNet.resnet18(n_classes=2).to(device)

criterion = torch.nn.CrossEntropyLoss().to(device)  
optimizer = torch.optim.Adam(model.parameters(), lr= lr, weight_decay=0.001)

for epoch in range(epochs):
    avg_cost = 0
    train_loss = 0
    train_total = 0
    train_correct = 0
    model.train()
    for X, Y in trainloader: 
        X = X.to(device)
        Y = Y.to(device) 
        
        optimizer.zero_grad() 

        model_predict  = model(X)
        loss = criterion(model_predict, Y) 
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        _, predicted = model_predict.max(1)
        train_total += Y.size(0) 
        train_correct += predicted.eq(Y).sum().item()
    train_acc = train_correct / train_total
    valid_loss = 0
    valid_total = 0
    valid_correct = 0
    model.eval() 
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, labels)
            valid_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            valid_total += labels.size(0)
            valid_correct += (predicted == labels).sum().item()
    valid_acc = valid_correct / valid_total
    print('[%d/%d] TrainLoss: %.3f, ValLoss: %.3f | TrainAcc: %.2f, ValAcc: %.2f' % (epoch, epochs, train_loss, valid_loss, train_acc, valid_acc))

import torch 
torch.save(model.state_dict(), './Resnet18_for_MSIMSS.pt')

"""## Train data Set 늘리고 실험"""

from glob import glob
import random
from matplotlib import pyplot as plt
import torchvision
import numpy as np
import torch
from torchvision import transforms 

SEED = 42
random.seed(SEED)

DATA_PATH_TRAIN_LIST = glob('./TCGA_DATA/CRC_TRAIN/*/*.png') 

MSS_TEST_LIST = glob('./TCGA_DATA/CRC_TEST/MSS/*.png')
random.shuffle(MSS_TEST_LIST)
MSS_TEST_LIST = MSS_TEST_LIST[:28335]
random.shuffle(MSS_TEST_LIST)

MSI_TEST_LIST = glob('./TCGA_DATA/CRC_TEST/MSIMUT/*.png')
random.shuffle(MSI_TEST_LIST)  


DATA_PATH_TEST_LIST = MSS_TEST_LIST[:1000] + MSI_TEST_LIST[:1000]
DATA_PATH_TRAIN_LIST += MSS_TEST_LIST[1000:] 
DATA_PATH_TRAIN_LIST += MSI_TEST_LIST[1000:] 


os.environ['PYTHONHASHSEED'] = str(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

lr = 0.0001
epochs = 40
batch_size = 100

from torch.utils.data import Dataset, DataLoader
from skimage import io, transform

class CRC_DataSet(Dataset):
    #data_path_list - 이미지 path 전체 리스트
    #label - 이미지 ground truth
    def __init__(self, data_path_list, classes, transform=None):
        self.path_list = data_path_list
        self.label = []
        for path in data_path_list:
            self.label.append(path.split('/')[-2])
        self.transform = transform
        self.classes = classes
    
    def __len__(self):
        return len(self.path_list)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        image = io.imread(self.path_list[idx])
        if self.transform is not None:
            image = self.transform(image)
        return image, self.classes.index(self.label[idx])

import torch
from torchvision import transforms 

classes = ('MSIMUT', 'MSS')

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TRAIN_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

testloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TEST_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

model = ResNet.resnet50(n_classes= 2).to(device)

criterion = torch.nn.CrossEntropyLoss().to(device)  
optimizer = torch.optim.Adam(model.parameters(), lr= lr, weight_decay=0.1) 
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=10, eta_min=0.00001)

best_valid_loss = 10000
for epoch in range(epochs):
    avg_cost = 0
    train_loss = 0
    train_total = 0
    train_correct = 0
    model.train()
    for X, Y in trainloader: 
        X = X.to(device)
        Y = Y.to(device) 
        
        optimizer.zero_grad() 

        model_predict  = model(X)
        loss = criterion(model_predict, Y) 
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        _, predicted = model_predict.max(1)
        train_total += Y.size(0) 
        train_correct += predicted.eq(Y).sum().item()
    scheduler.step()
    train_acc = train_correct / train_total
    valid_loss = 0
    valid_total = 0
    valid_correct = 0
    model.eval() 
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = F.cross_entropy(outputs, labels)
            valid_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            valid_total += labels.size(0)
            valid_correct += (predicted == labels).sum().item()
    valid_acc = valid_correct / valid_total 
    if best_valid_loss > valid_loss:
      torch.save(model.state_dict(), './resnet50_best_with_lr_schedular.pt') 
      best_valid_loss = valid_loss
    print('[%d/%d] TrainLoss: %.3f, ValLoss: %.3f | TrainAcc: %.2f, ValAcc: %.2f' % (epoch, epochs, train_loss, valid_loss, train_acc, valid_acc))

# colab memory 관리
import gc
gc.collect()
torch.cuda.empty_cache()

"""## Deep tuning with Best Model"""

best_model = ResNet.resnet50(n_classes= 2).to(device)

best_model.load_state_dict(torch.load('./resnet50_the_best.pt'))

lr = 0.0001
epochs = 20
batch_size = 10

import torch
from torchvision import transforms 

classes = ('MSIMUT', 'MSS')

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TRAIN_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

testloader = torch.utils.data.DataLoader(
    CRC_DataSet(
        DATA_PATH_TEST_LIST, 
        classes,
        transform=transform
    ),
    batch_size=batch_size,
    shuffle = True
)

criterion = torch.nn.CrossEntropyLoss().to(device)  
optimizer = torch.optim.Adam(best_model.parameters(), lr= lr, weight_decay=0.1) 
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience= 3, mode = 'min')

best_valid_loss = 10000
for epoch in range(epochs):
    avg_cost = 0
    train_loss = 0
    train_total = 0
    train_correct = 0
    best_model.train()
    for X, Y in trainloader: 
        X = X.to(device)
        Y = Y.to(device) 
        
        optimizer.zero_grad() 

        model_predict  = best_model(X)
        loss = criterion(model_predict, Y) 
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        _, predicted = model_predict.max(1)
        train_total += Y.size(0) 
        train_correct += predicted.eq(Y).sum().item()
    train_acc = train_correct / train_total
    valid_loss = 0
    valid_total = 0
    valid_correct = 0
    best_model.eval() 
    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = best_model(inputs)
            loss = F.cross_entropy(outputs, labels)
            valid_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            valid_total += labels.size(0)
            valid_correct += (predicted == labels).sum().item()
    valid_acc = valid_correct / valid_total 
    torch.save(best_model.state_dict(), './resnet50_the_best.pt') 
    print('[%d/%d] TrainLoss: %.3f, ValLoss: %.3f | TrainAcc: %.2f, ValAcc: %.2f' % (epoch, epochs, train_loss, valid_loss, train_acc, valid_acc))
    scheduler.step(valid_loss)

len(testloader)

valid_loss = 0
valid_total = 0
valid_correct = 0
for inputs, labels in testloader:
  inputs, labels = inputs.to(device), labels.to(device)
  outputs = best_model(inputs)
  loss = F.cross_entropy(outputs, labels)
  valid_loss += loss.item()
  _, predicted = torch.max(outputs.data, 1)
  valid_total += labels.size(0)
  valid_correct += (predicted == labels).sum().item()
valid_acc = valid_correct / valid_total

valid_acc

from glasses.interpretability import GradCam, SaliencyMap
from torchvision.transforms import Normalize
dataiter = iter(trainloader) 
images, labels = dataiter.next() 
plt.imshow(torchvision.utils.make_grid(images[0], normalize= True).permute(1,2,0)) 
_ = best_model.interpret(images[0].to(device).unsqueeze(0), using=GradCam()).show()

output = best_model(images.to(device)) 
_, predicted = torch.max(output.data, 1)
print('실제 라벨: {} , 예측 라벨: {}'.format(labels[0], predicted[0]))